# Activation Function

## Activation Functions

Activation function or transfer function is used to determine the output of a neuron or node. It is a mathematical gate in between the input feeding the current neuron and its output going to the next layer.

![](../../../.gitbook/assets/image%20%2878%29.png)

In deep learning,  we commonly use non-linear Activation Function include

* Sigmoid:  Output limit to \[0 1\]. But gives gradient vanishing problem, not used anymore 
* ReLU\(rectified linear unit\): most commonly used in CNN \(hidden layers\) 
* Others: Tanh, Leaky ReLU, Maxout...

![Image from MIT Deeplearning Lecture](../../../.gitbook/assets/image%20%2883%29.png)

![Cheat sheet of commonly used Activation Function ](../../../.gitbook/assets/image%20%2877%29.png)

### Further Reading






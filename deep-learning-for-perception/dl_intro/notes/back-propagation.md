# Activation Function

## Activation Functions

Activation function or transfer function is used to determine the output of a neuron or node. It is a mathematical gate in between the input feeding the current neuron and its output going to the next layer.

![](https://github.com/ykkimhgu/DLIP_doc/tree/3298e5d2a4b6369e5cef7973dd93eef44ca7addf/.gitbook/assets/image%20%2878%29.png)

In deep learning, we commonly use non-linear Activation Function include

* Sigmoid:  Output limit to \[0 1\]. But gives gradient vanishing problem, not used anymore 
* ReLU\(rectified linear unit\): most commonly used in CNN \(hidden layers\) 
* Others: Tanh, Leaky ReLU, Maxout...

![Image from MIT Deeplearning Lecture](https://github.com/ykkimhgu/DLIP_doc/tree/3298e5d2a4b6369e5cef7973dd93eef44ca7addf/.gitbook/assets/image%20%2883%29.png)

![Cheat sheet of commonly used Activation Function ](https://github.com/ykkimhgu/DLIP_doc/tree/3298e5d2a4b6369e5cef7973dd93eef44ca7addf/.gitbook/assets/image%20%2877%29.png)

### Further Reading

